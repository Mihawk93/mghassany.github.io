<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="" />
<meta property="og:type" content="book" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="">

<title></title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="css\style.css" type="text/css" />
<link rel="stylesheet" href="css\toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Unsupervised Learning</b></span></li>
<li class="has-sub"><a href="1-dimensionality-reduction.html#dimensionality-reduction"><span class="toc-section-number">1</span> Dimensionality Reduction</a><ul>
<li><a href="1-1-unsupervised-learning.html#unsupervised-learning"><span class="toc-section-number">1.1</span> Unsupervised Learning</a></li>
<li><a href="1-2-principal-components-analysis.html#principal-components-analysis"><span class="toc-section-number">1.2</span> Principal Components Analysis</a></li>
<li class="has-sub"><a href="1-3-principal-components.html#principal-components"><span class="toc-section-number">1.3</span> Principal Components</a><ul>
<li><a href="1-3-principal-components.html#notations-and-procedure">Notations and Procedure</a></li>
<li><a href="1-3-principal-components.html#first-principal-component-textpc_1-y_1">First Principal Component (<span class="math inline">\(\text{PC}_1\)</span>): <span class="math inline">\(Y_1\)</span></a></li>
<li><a href="1-3-principal-components.html#second-principal-component-textpc_2-y_2">Second Principal Component (<span class="math inline">\(\text{PC}_2\)</span>): <span class="math inline">\(Y_2\)</span></a></li>
<li><a href="1-3-principal-components.html#ith-principal-component-textpc_i-y_i"><span class="math inline">\(i^{th}\)</span> Principal Component (<span class="math inline">\(\text{PC}_i\)</span>): <span class="math inline">\(Y_i\)</span></a></li>
</ul></li>
<li class="has-sub"><a href="1-4-how-do-we-find-the-coefficients.html#how-do-we-find-the-coefficients"><span class="toc-section-number">1.4</span> How do we find the coefficients?</a><ul>
<li><a href="1-4-how-do-we-find-the-coefficients.html#why-it-may-be-possible-to-reduce-dimensions">Why It May Be Possible to Reduce Dimensions</a></li>
<li><a href="1-4-how-do-we-find-the-coefficients.html#procedure">Procedure</a></li>
</ul></li>
<li><a href="1-5-standardization-of-the-features.html#standardization-of-the-features"><span class="toc-section-number">1.5</span> Standardization of the features</a></li>
<li class="has-sub"><a href="1-6-projection-of-the-data.html#projection-of-the-data"><span class="toc-section-number">1.6</span> Projection of the data</a><ul>
<li><a href="1-6-projection-of-the-data.html#scores">Scores</a></li>
<li><a href="1-6-projection-of-the-data.html#visualization">Visualization</a></li>
<li><a href="1-6-projection-of-the-data.html#extra">Extra</a></li>
</ul></li>
<li class="has-sub"><a href="1-7-case-study.html#case-study"><span class="toc-section-number">1.7</span> Case study</a><ul>
<li><a href="1-7-case-study.html#employement-in-european-countries-in-the-late-70s">Employement in European countries in the late 70s</a></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="principal-components" class="section level2">
<h2><span class="header-section-number">1.3</span> Principal Components</h2>
<div id="notations-and-procedure" class="section level3 unnumbered">
<h3>Notations and Procedure</h3>
<p>Suppose that we have a random vector of the features <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[ \textbf{X} = \left(\begin{array}{c} X_1\\ X_2\\ \vdots \\X_p\end{array}\right) \]</span></p>
<p>with population variance-covariance matrix</p>
<p><span class="math display">\[ \text{var}(\textbf{X}) = \Sigma = \left(\begin{array}{cccc}\sigma^2_1 &amp; \sigma_{12} &amp; \dots &amp;\sigma_{1p}\\ \sigma_{21} &amp; \sigma^2_2 &amp; \dots &amp;\sigma_{2p}\\  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \sigma_{p1} &amp; \sigma_{p2} &amp; \dots &amp; \sigma^2_p\end{array}\right) \]</span></p>
<p>Consider the linear combinations</p>
<p><span class="math display">\[ \begin{array}{lll} Y_1 &amp; = &amp; a_{11}X_1 + a_{12}X_2 + \dots + a_{1p}X_p \\ Y_2 &amp; = &amp; a_{21}X_1 + a_{22}X_2 + \dots + a_{2p}X_p \\ &amp; &amp; \vdots \\ Y_p &amp; = &amp; a_{p1}X_1 + a_{p2}X_2 + \dots +a_{pp}X_p \end{array} \]</span></p>
<p>Note that <span class="math inline">\(Y_i\)</span> is a function of our random data, and so is also random. Therefore it has a population variance</p>
<p><span class="math display">\[ \text{var}(Y_i) = \sum_{k=1}^{p} \sum_{l=1}^{p} a_{ik} a_{il} \sigma_{kl} = \mathbf{a}^T_i \Sigma \mathbf{a}_i \]</span></p>
<p>Moreover, <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> will have a population covariance</p>
<p><span class="math display">\[ \text{cov}(Y_i, Y_j) = \sum_{k=1}^{p} \sum_{l=1}^{p} a_{ik}a_{jl}\sigma_{kl} = \mathbf{a}^T_i\Sigma\mathbf{a}_j \]</span></p>
<p>and a correlation</p>
<p><span class="math display">\[ \text{cor}(Y_i, Y_j) = \frac{\text{cov}(Y_i, Y_j)}{\sigma^2_i \sigma^2_j}\]</span></p>
<p>Here the coefficients <span class="math inline">\(a_{ij}\)</span> are collected into the vector</p>
<p><span class="math display">\[ \mathbf{a}_i = \left(\begin{array}{c} a_{i1}\\ a_{i2}\\ \vdots \\ a_{ip}\end{array}\right) \]</span></p>
<p>The coefficients <span class="math inline">\(a_{ij}\)</span> are also called <em>loadings</em> of the principal component <span class="math inline">\(i\)</span> and <span class="math inline">\(\mathbf{a}_i\)</span> is a principal component loading vector.</p>
<div class="rmdtip">
<ul>
<li>
The total variation of <span class="math inline"><em>X</em></span> is the <em>trace</em> of the variance-covariance matrix <span class="math inline"><em>Σ</em></span>.
</li>
<li>
The trace of <span class="math inline"><em>Σ</em></span> is the sum of the variances of the individual variables.
</li>
<li>
<span class="math inline"><em>t</em><em>r</em><em>a</em><em>c</em><em>e</em>(<em>Σ</em>)  =  <em>σ</em><sub>1</sub><sup>2</sup> + <em>σ</em><sub>2</sub><sup>2</sup> + … + <em>σ</em><sub><em>p</em></sub><sup>2</sup></span>
</li>
</ul>
</div>
</div>
<div id="first-principal-component-textpc_1-y_1" class="section level3 unnumbered">
<h3>First Principal Component (<span class="math inline">\(\text{PC}_1\)</span>): <span class="math inline">\(Y_1\)</span></h3>
<p>The <em>first principal component</em> is the <em>normalized</em> linear combination of the features <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span> that has maximum variance (among all linear combinations), so it accounts for as much variation in the data as possible.</p>
<p>Specifically we will define coefficients <span class="math inline">\(a_{11},a_{12},\ldots,a_{1p}\)</span> for that component in such a way that its variance is maximized, subject to the constraint that the sum of the squared coefficients is equal to one (that is what we mean by <em>normalized</em>). This constraint is required so that a unique answer may be obtained.</p>
<p>More formally, select <span class="math inline">\(a_{11},a_{12},\ldots,a_{1p}\)</span> that maximizes</p>
<p><span class="math display">\[ \text{var}(Y_1) = \mathbf{a}^T_1\Sigma\mathbf{a}_1  = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{1k}a_{1l}\sigma_{kl} \]</span></p>
<p>subject to the constraint that</p>
<p><span class="math display">\[ \sum_{j=1}^{p}a^2_{1j} = \mathbf{a}^T_1\mathbf{a}_1   = 1 \]</span></p>
</div>
<div id="second-principal-component-textpc_2-y_2" class="section level3 unnumbered">
<h3>Second Principal Component (<span class="math inline">\(\text{PC}_2\)</span>): <span class="math inline">\(Y_2\)</span></h3>
<p>The <em>second principal component</em> is the linear combination of the features <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span> that accounts for as much of the remaining variation as possible, with the constraint that the correlation between the first and second component is 0. So the second principal component has maximal variance out of all linear combinations that are uncorrelated with <span class="math inline">\(Y_1\)</span>.</p>
<p>To compute the coefficients of the second principal component, we select <span class="math inline">\(a_{21},a_{22},\ldots,a_{2p}\)</span> that maximizes the variance of this new component</p>
<p><span class="math display">\[\text{var}(Y_2) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{2k}a_{2l}\sigma_{kl} = \mathbf{a}^T_2\Sigma\mathbf{a}_2 \]</span></p>
<p>subject to:</p>
<ul>
<li><p>The constraint that the sums of squared coefficients add up to one, <span class="math inline">\(\sum_{j=1}^{p}a^2_{2j} = \mathbf{a}^T_2\mathbf{a}_2 = 1\)</span>.</p></li>
<li><p>Along with the additional constraint that these two components will be uncorrelated with one another:</p></li>
</ul>
<p><span class="math display">\[ \text{cov}(Y_1, Y_2) = \mathbf{a}^T_1\Sigma\mathbf{a}_2  = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{1k}a_{2l}\sigma_{kl} = 0 \]</span></p>
<p>All subsequent principal components have this same property: they are linear combinations that account for as much of the remaining variation as possible and they are not correlated with the other principal components.</p>
<p>We will do this in the same way with each additional component. For instance:</p>
</div>
<div id="ith-principal-component-textpc_i-y_i" class="section level3 unnumbered">
<h3><span class="math inline">\(i^{th}\)</span> Principal Component (<span class="math inline">\(\text{PC}_i\)</span>): <span class="math inline">\(Y_i\)</span></h3>
<p>We select <span class="math inline">\(a_{i1},a_{i2},\ldots,a_{ip}\)</span> that maximizes</p>
<p><span class="math display">\[ \text{var}(Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{ik}a_{il}\sigma_{kl} = \mathbf{a}^T_i\Sigma\mathbf{a}_i \]</span></p>
<p>subject to the constraint that the sums of squared coefficients add up to one, along with the additional constraint that this new component will be uncorrelated with all the previously defined components:</p>
<p><span class="math display">\[ \sum_{j=1}^{p}a^2_{ij} \mathbf{a}^T_i\mathbf{a}_i = \mathbf{a}^T_i\mathbf{a}_i = 1\]</span></p>
<p><span class="math display">\[ \text{cov}(Y_1, Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{1k}a_{il}\sigma_{kl} = \mathbf{a}^T_1\Sigma\mathbf{a}_i = 0 \]</span></p>
<p><span class="math display">\[\text{cov}(Y_2, Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{2k}a_{il}\sigma_{kl} = \mathbf{a}^T_2\Sigma\mathbf{a}_i = 0\]</span> <span class="math display">\[\vdots\]</span> <span class="math display">\[\text{cov}(Y_{i-1}, Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{i-1,k}a_{il}\sigma_{kl} = \mathbf{a}^T_{i-1}\Sigma\mathbf{a}_i = 0\]</span></p>
<p>Therefore all principal components are uncorrelated with one another.</p>
</div>
</div>
<p style="text-align: center;">
<a href="1-2-principal-components-analysis.html"><button class="btn btn-default">Previous</button></a>
<a href="1-4-how-do-we-find-the-coefficients.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
<!-- </html> -->
