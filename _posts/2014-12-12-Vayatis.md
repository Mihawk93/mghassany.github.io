---
layout: post
title: "Vayatis -- 2.1 -- Joaillerie"
subtitle: test
date: 2014-12-12 15:34:25
--- 

 
 * TOC
{:toc}
 
 
 

{% highlight r %}
# to calculate time spent
# ptm <- proc.time()
data <- read.csv("~/Dropbox/TEM/CuratedMedia/source_separation/Joaillerie.csv",sep=",",fileEncoding = "UTF-8",header=T)
data <- data.table(data)
 
# proc.time() - ptm
 
# fread is equivalent to read.csv but much faster and loads as data.table (from data.table library). sep is automatic and header is automatic
 
#data <- fread("~/Dropbox/TEM/CuratedMedia/source_separation/Joaillerie.csv")
{% endhighlight %}
 
 
Précédemment j'ai fait quelques analyses sur le cas 'Banque'. Dans ce rapport je ferai une analyse sur le cas 'Joaillerie'.
 
A noter avant de commencer que LinkFluence ont mi à jour leur plateforme Radarly pendant cette semaine. Ils ont changé l'interface.
J'ai téléchargé les données de clusters du corpus Joaillerie.
 
**Tout d'abord**, une surprise inattendue après avoir téléchargé les données, ajouté les ids des clusters, et mergé les fichiers. En important le tableau Joaillerie dans R, je remarque que le nombre de colonnes dans ce tableau est 63, contre 27 dans les anciennes versions de Radarly (donc les données Banque téléchargés).
 
Et les noms de variables ne sont le mêmes que les anciennes variables.
Ce qui m'oblige à re editer le code des anciennes analyses pour traiter maintenant Joaillerie.
 
Voici d'abord la liste des variables dans ce tableau : 
 

{% highlight r %}
colnames(data)
{% endhighlight %}



{% highlight text %}
 [1] "date"                        "text"                       
 [3] "title"                       "permalink"                  
 [5] "site.name"                   "queries"                    
 [7] "lang"                        "tone"                       
 [9] "platform"                    "post.type"                  
[11] "author"                      "screen.name"                
[13] "gender"                      "avatar"                     
[15] "inferred.country"            "inferred.region"            
[17] "inferred.city"               "inferred.longitude"         
[19] "inferred.latitude"           "inferred.country.population"
[21] "inferred.region.population"  "inferred.city.population"   
[23] "declared.country"            "declared.region"            
[25] "declared.city"               "declared.longitude"         
[27] "declared.latitude"           "declared.country.population"
[29] "declared.region.population"  "declared.city.population"   
[31] "twitter.author.id"           "twitter.verified.author"    
[33] "twitter.followers"           "twitter.retweets"           
[35] "twitter.retweed.of"          "twitter.replies"            
[37] "facebook.author.id"          "facebook.type"              
[39] "facebook.fans"               "instagram.author.id"        
[41] "instagram.followers"         "web.pagerank"               
[43] "web.inbound.links"           "web.shares.on.twitter"      
[45] "web.comments"                "impressions"                
[47] "estimated.reach"             "embedded.url"               
[49] "media.type"                  "media.url"                  
[51] "favorite"                    "keywords"                   
[53] "hashtags"                    "mentions"                   
[55] "named.entities"              "story"                      
[57] "topic"                       "facebook.likes"             
[59] "youtube.dislikes"            "facebook.shares"            
[61] "comments"                    "twitter.favorites"          
[63] "clusterID"                  
{% endhighlight %}
 
Quelques nouvelles variables qui pourront être intéressantes : 
 
- `site.name`
- `post.type`
- `author` & `screen.name` (nom d'utilisateur et son identifiant)
- `gender` (estimation du genre (M F))
- `avatar` (lien url du photo  du compte)
- `inferred.longitude` & `inferred.latitude` & `infered.country.popoulation` (si on a la localisation, dans la variable `inferred.country`)
- `twitter.followers` & `facebook.fans` & autres (réactions par plateforme)
- `web.pagerank` (si un site web, son pagerank)
- `story` (celle ci c'est l'id du cluster, donc les ids que j'ajoute manuellement, l'avantage d'avoir cela c'est de pouvoir demander de télécharger tous les données joaillerie en un seul coup, ces données vont inclure cette variable et on a donc tous les clusters).
 
<!-- # Montrons d'abord quelques ASI (clusters) -->
 
<!-- Les 20 premiers clusters :  -->
 
<!-- ```{r} -->
<!-- for (i in 1:20) { -->
 
<!--   subset <- data[clusterID == i] -->
 
<!--   #extraction de dates pour créer ASI (Activity Signal of Interest) -->
<!--   dates <- as.POSIXct(subset$date,format="%d/%m/%Y %H:%M") -->
<!--   rounded_dates <- round(dates , "hour") -->
<!--   rounded_dates <- as.POSIXct(rounded_dates) -->
<!--   Period_days <- difftime(max(rounded_dates),min(rounded_dates)) -->
<!--   Period_days <- round(Period_days,2) -->
<!--   Period_hours <- difftime(max(rounded_dates),min(rounded_dates), units = "hours") -->
<!--   Period_hours <- round(Period_hours,2) -->
<!--   Occurences <- length(rounded_dates) -->
 
<!--   freq <- plyr::count(rounded_dates) -->
 
<!--   df1.zoo<-zoo(freq[,-1],freq[,1]) #set date to Index -->
 
<!--   df2 <- merge(df1.zoo,zoo(,seq(start(df1.zoo),end(df1.zoo),by="hour")), all=TRUE) -->
 
<!--   ASI <- data.frame(Date=time(df2), df2, check.names=FALSE, row.names=NULL) -->
<!--   colnames(ASI) <- c("date", "freq") -->
<!--   rm(df1.zoo,df2) -->
<!--   ASI[is.na(ASI)] <- 0 -->
 
<!--   # plot(ASI$date,ASI$freq,type="b",col="blue",xlab="",ylab="") -->
<!--   # mtext(paste("ASI",i,"\n","T = ",Period_days, " days \n","#Occurences = ", Occurences, " posts" )) -->
<!--   #  -->
<!--   print( -->
<!--   ggplot(ASI, aes(x=as.character(date), y=freq,group=1)) + -->
<!--     geom_line(color="blue") + -->
<!--     geom_point(color="blue") + -->
<!--     expand_limits(y=0) + -->
<!--     theme_minimal() + -->
<!--     ggtitle(paste("ASI of cluster",i,sep=" ")) + -->
<!--     xlab("time") + -->
<!--     theme(axis.text.x = element_blank())  + -->
<!--     annotate("text", color= "blue", size = 5, x=Inf, y = Inf, label = paste("T = ",Period_days, " days \n", "T = ", Period_hours, " hours \n", "#Occurences = ", Occurences, " posts" ), vjust=1, hjust=1) -->
<!-- ) -->
 
<!-- } -->
<!-- ``` -->
 
<!-- > Je remarque déjà qu'il y a plusieurs profils de cascade. Espérons avoir des bons groupes en faisant la classification. -->
 
# Rappel : Les variables calculées 
 
Les variables calculées en première version : 
 
For every ASI : 
 
$s_c$ is an ASI discrete time signal of length $T$ for the cascade $c$.
 
- **sumAct** : sum of total activity (number of meme posts)
- **maxAct** : max activity level 
- **meanAct** : mean activity level( sumAct/T )
- ~~**median-to-mean** : median($s_c$)/meanAct~~ (**Remarque**:j'obtiens toujorus des 0) **Laissé tomber**
- **varAct** : variance of the activity level $$\frac{1}{T}\sum_T (meanAct - s_c(t))^2$$
- **actLen** : total activity length, the time between the first and the last observed activity in the signal $s_c$. (**in hours ?**)
- **peaksNum/actLen** : the ratio of the number of peaks existing in the signal to its active length. As a peak is identified a single value of the signal that is significantly higher than its two neighboring values, the one before and the one after it.
- **maxZeroIntval/actLen** : the ratio of the longest pause of signal where we observe zero values during the active length to that length (actLen).
- **maxNonZeroIntval/actLen** : same as in maxZeroIntval/actLen but for the longest non-zero part of the signal.
 
- **$$V_c$$** : number of activated node = $$|G_c|$$.
 
- **reactions** : somme de likes +  shares + retweets + comments + replies + favorites
 
- **sumImp** : la somme des impressions (distinct users)
 
Ratios Platformes : 
 
- **ratioTwitter** 
- **ratioFacebook** 
- **ratioMedia** 
- **ratioBlog** 
- **ratioWebsite** 
 
Les variables qu'on souhaite ajouter dans cette version sont : 
 
Sentiment analysis :
 
- **Sentiment** : $$\frac{\# negative}{sumAct} \times (-1)$ + $\frac{\# positive}{sumAct} \times (1)$$
 
<!-- ~~Corpus~~ :  -->
 
<!-- - **ratioLaBanquePostale**  -->
<!-- - **ratioCreditMutuel**  -->
<!-- - **ratioSociétéGénérale**  -->
<!-- - **ratioCIC**  -->
<!-- - **ratioCreditAgricole**  -->
<!-- - **ratioBNPParibas**  -->
<!-- - **ratioBanquePopulaire**  -->
<!-- - **ratioINGDirect**  -->
 
 

{% highlight r %}
# corpusJoaillerie <- c("LaBanquePostale","CreditMutuel","SociétéGénérale","CIC","CreditAgricole","BNPParibas","BanquePopulaire","INGDirect")
 
#création d'une matrice clusters pour faire ACP et Kmeans de ces clusters
# clusters <- data.frame(clusterId= integer(0), sumAct= integer(0), maxAct = integer(0), meanAct=numeric(0),varAct = numeric(0),actLen = numeric(0),peaksNum_actLen = numeric(0),maxZeroIntval_actLen = numeric(0),maxNonZeroIntval_actLen = numeric(0),V_c = numeric(0),reactions = numeric(0),sumImp = numeric(0),sentiment = numeric(0),ratioLaBanquePostale = numeric(0),ratioCreditMutuel = numeric(0),ratioSociétéGénérale = numeric(0),ratioCIC = numeric(0),ratioCreditAgricole = numeric(0),ratioBNPParibas = numeric(0),ratioBanquePopulaire = numeric(0),ratioINGDirect = numeric(0),ratioTwitter = numeric(0),ratioFacebook = numeric(0),ratioMedia = numeric(0),ratioBlog = numeric(0),ratioWebsite = numeric(0))
 
clusters <- data.frame(clusterId= integer(0), sumAct= integer(0), maxAct = integer(0), meanAct=numeric(0),varAct = numeric(0),actLen = numeric(0),peaksNum_actLen = numeric(0),maxZeroIntval_actLen = numeric(0),maxNonZeroIntval_actLen = numeric(0),V_c = numeric(0),reactions = numeric(0),sumImp = numeric(0),sentiment = numeric(0),ratioTwitter = numeric(0),ratioFacebook = numeric(0),ratioMedia = numeric(0),ratioBlog = numeric(0),ratioWebsite = numeric(0))
 
# ids <- c(1:49,51:100)
ids <- 1:100
 
for(i in ids){
 
  #subset du cluster i
  subset <- data[clusterID == i]
 
  #extraction de dates pour créer ASI (Activity Signal of Interest)
  dates <- as.POSIXct(subset$date,format="%d/%m/%Y %H:%M")
  rounded_dates <- round(dates , "hour")
  rounded_dates <- as.POSIXct(rounded_dates)
  Period_days <- difftime(max(rounded_dates),min(rounded_dates))
  Period_days <- round(Period_days,2)
  Period_hours <- difftime(max(rounded_dates),min(rounded_dates), units = "hours")
  Period_hours <- round(Period_hours,2)
  Occurences <- length(rounded_dates)
 
  # for ASI that happen in the same hour 
  if(Period_hours == 0){
    Period_hours <- 1
  }
  freq <- plyr::count(rounded_dates)
 
  df1.zoo<-zoo(freq[,-1],freq[,1]) #set date to Index
 
  df2 <- merge(df1.zoo,zoo(,seq(start(df1.zoo),end(df1.zoo),by="hour")), all=TRUE)
 
  ASI <- data.frame(Date=time(df2), df2, check.names=FALSE, row.names=NULL)
  colnames(ASI) <- c("date", "freq")
  rm(df1.zoo,df2)
  ASI[is.na(ASI)] <- 0
 
 
  #calcul de sumAct
  sumAct <- nrow(subset)
 
  #calcul de maxAct
  maxAct <- max(ASI$freq)
 
  #calcul de "meanAct"
  meanAct <- sumAct/as.numeric(Period_hours)
 
  #calcul de "median-to-mean"
  median_to_mean <- median(ASI$freq)/meanAct
 
  #calcul de "varAct"
  if(Period_hours == 1){
    varAct <- 0
  } else {
  varAct <- var(ASI$freq)
  }
  #calcul de "actLen"
  actLen <- as.numeric(Period_hours)
 
  #calcul de "peaksNum/actLen"
  require(quantmod)
  peaksNum_actLen <- length(findPeaks(ASI$freq))/actLen
 
  #calcul de "maxZeroIntval/actLen"
  if(Period_hours == 1){
    maxZeroIntval_actLen <- 0
  } else {
  runs <- rle(ASI$freq)
  maxZeroIntval <- max(runs$lengths[runs$values==0])
  maxZeroIntval_actLen <- maxZeroIntval/actLen
  }
 
 
  #calcul de "maxNonZeroIntval/actLen"
  runs <- rle(ASI$freq>0)
  maxNonZeroIntval <- max(runs$lengths[runs$values==T])
  maxNonZeroIntval_actLen <- maxNonZeroIntval/actLen
 
  #calcul de "$V_c$" numbers of nodes (distinct authors in subset)
  V_c <- length(unique(subset$author))
 
  #calcul de "reactions"     #A CORRIGER
  reactions <- sum(subset$twitter.retweets,na.rm=T) + sum(subset$twitter.replies,na.rm=T) + sum(subset$twitter.favorites,na.rm=T) + sum(subset$web.shares.on.twitter,na.rm=T) + sum(subset$web.comments,na.rm=T) + sum(subset$facebook.likes,na.rm=T) + sum(subset$facebook.shares,na.rm=T) + sum(subset$comments,na.rm=T)
    
 
  #calcul de "sumImp"
  sumImp <- sum(subset$impressions,na.rm=T)
 
  #calcul de "sumReach"
  sumReach <- sum(subset$estimated.reach,na.rm=T)
 
  #calcul de "meanImp"
  meanImp <- mean(subset$impressions,na.rm=T)
 
  #calcul de "meanReach"
  meanReach <- mean(subset$estimated.reach,na.rm=T)
 
  #calcul de sentiment analysis (transformation de 3 facteur negative neutral positive)
 
  sentiment <- (sum(subset$tone == "positive")/length(subset$tone))*(1) + (sum(subset$tone == "negative")/length(subset$tone))*(-1)
 
 
  #Calcul des ratios des Banques
 
  queries.single <- character()
  for (jj in 1:nrow(subset))
  {
    temp <- strsplit(as.character(subset$queries[jj]),";")
    queries.single <- append(queries.single, temp[[1]])
  }
 
  # ratioLaBanquePostale <- sum(queries.single==corpusBanque[1])/sumAct
  # ratioCreditMutuel <- sum(queries.single==corpusBanque[2])/sumAct
  # ratioSociétéGénérale <- sum(queries.single==corpusBanque[3])/sumAct
  # ratioCIC <- sum(queries.single==corpusBanque[4])/sumAct
  # ratioCreditAgricole <- sum(queries.single==corpusBanque[5])/sumAct
  # ratioBNPParibas <- sum(queries.single==corpusBanque[6])/sumAct
  # ratioBanquePopulaire <- sum(queries.single==corpusBanque[7])/sumAct
  # ratioINGDirect <- sum(queries.single==corpusBanque[8])/sumAct
 
  # rm(temp,queries.single)
 
  #Plateformes
 
  Twitter <- nrow(subset[platform=="Twitter"])
  Facebook <- nrow(subset[platform=="Facebook"])
  Media <- nrow(subset[platform=="Media"])
  Blog <- nrow(subset[platform=="Blog"])
  Website <- nrow(subset[platform=="Website"])
 
  ratioTwitter <- Twitter/nrow(subset)
  ratioFacebook <- Facebook/nrow(subset)
  ratioMedia <- Media/nrow(subset)
  ratioBlog <- Blog/nrow(subset)
  ratioWebsite <- Website/nrow(subset)
 
  #insertion dans la matrice finale
 
      clusters[i,] <- c(i,sumAct,maxAct,meanAct,varAct,actLen,peaksNum_actLen,maxZeroIntval_actLen,maxNonZeroIntval_actLen,V_c,reactions,sumImp,sentiment,ratioTwitter,ratioFacebook,ratioMedia,ratioBlog,ratioWebsite)
 
 
 
  }
 
# rm(ASI,freq,subset)
 
#write.csv pour montrer les fichiers des clusters
 
# write.csv(clusters, file="clusters.csv", sep=",",fileEncoding = "UTF-8",row.names = FALSE)
 
 
# print(
#   ggplot(ASI, aes(x=as.character(date), y=freq,group=1)) +
#     geom_line(color="blue") +
#     geom_point(color="blue") +
#     expand_limits(y=0) +
#     theme_minimal() +
#     ggtitle(paste("ASI of cluster",id,sep=" ")) +
#     xlab("time") +
#     theme(axis.text.x = element_text(angle=90))  +
#     annotate("text", color= "blue", size = 5, x=Inf, y = Inf, label = paste("T = ",Period_days, " days \n", "T = ", Period_hours, " hours \n", "#Occurences = ", Occurences, " posts" ), vjust=1, hjust=1)
# )
{% endhighlight %}
 
 
<!-- Le tableau `clusters` est téléchargeable sur le lien suivant : [clusters.csv](https://dl.dropboxusercontent.com/s/eo32nybuyuhosuq/clusters.csv?dl=0) -->
 
Alors j'ai un problème ici que je n'ai encore pu résoudre, c'est qu'il y a des clusters dont touts les posts sont la même heure ! du coup, on a des valeurs infinies et des champs nuls dans le tableau clusters et on ne peut pas procéder. 
Je trouverai une solution la semaine prochaine.
 
Un exemple est le cluster 70 : 
 

{% highlight r %}
clusters[70,]
{% endhighlight %}



{% highlight text %}
   clusterId sumAct maxAct meanAct varAct actLen peaksNum_actLen
70        70     77     77      77      0      1               0
   maxZeroIntval_actLen maxNonZeroIntval_actLen V_c reactions sumImp
70                    0                       1  76         1  10638
   sentiment ratioTwitter ratioFacebook ratioMedia ratioBlog ratioWebsite
70         0    0.8701299             0 0.02597403         0    0.1038961
{% endhighlight %}
 
> Solution : Dans ce cas , j'ai remplacé `Period_hours` (0 par 1), j'ai remplacé la Variance `varAct` (NA par 0, on obtiens NA parce qu'il y une seule heure), j'ai remplace `maxZeroIntval_actLen` (inf par 0)
 
# ACP
 
 

{% highlight r %}
library(corrplot)
corrplot(cor(clusters[,2:ncol(clusters)]), type="lower")
{% endhighlight %}

![plot of chunk unnamed-chunk-5](/figures/unnamed-chunk-5-1.png)
 
Commentaires sur cette figure : 
 
- ratioWebsite toujours corrélée négativement avec ratioTwitter (donc le clusters s'agissent soit de Twitter soit Website)
- remarquable corrélation positive entre ratioWebsite et actLen, et corrélation négative entre ratioTwitter et actLen. Pour moi ça veut dire que quand le continu du cluster est du website ça se partage pour une préiode plus longue sur le web que lorsque c'est du contenu twitter.
 
Voici les pourcentages que les composantes principales représentent
 

{% highlight r %}
#ACP
 
#scaling the matrix
clusters_scaled <- scale(clusters[,2:ncol(clusters)])
 
acp1 <- princomp(clusters_scaled, scores = TRUE, cor= TRUE)
summary(acp1)
{% endhighlight %}



{% highlight text %}
Importance of components:
                          Comp.1    Comp.2    Comp.3    Comp.4     Comp.5
Standard deviation     2.1340996 1.5048758 1.4079833 1.3243036 1.15416660
Proportion of Variance 0.2679048 0.1332148 0.1166128 0.1031635 0.07835886
Cumulative Proportion  0.2679048 0.4011196 0.5177323 0.6208958 0.69925470
                           Comp.6     Comp.7     Comp.8     Comp.9
Standard deviation     1.08203892 1.03534553 0.98354325 0.87907552
Proportion of Variance 0.06887107 0.06305532 0.05690337 0.04545728
Cumulative Proportion  0.76812577 0.83118109 0.88808446 0.93354174
                         Comp.10    Comp.11    Comp.12     Comp.13
Standard deviation     0.5639965 0.51558565 0.45318052 0.387522484
Proportion of Variance 0.0187113 0.01563697 0.01208074 0.008833746
Cumulative Proportion  0.9522530 0.96789001 0.97997075 0.988804496
                           Comp.14     Comp.15     Comp.16      Comp.17
Standard deviation     0.317925386 0.236117519 0.183001315 2.459530e-03
Proportion of Variance 0.005945679 0.003279499 0.001969969 3.558404e-07
Cumulative Proportion  0.994750176 0.998029675 0.999999644 1.000000e+00
{% endhighlight %}
 
Les 4 premiers axes principaux représentent 62% de données (pas mal)
Avec 8 valeurs propres nous arrivons à 88%.
Avec 10 valeurs propres nous arrivons à 95%.
 
<!-- Cette fois on n'a pas des bonnes pourcentages. -->
<!-- La première valeur propre (eigenvalue) interpréte 21%, la deuxième 11%, nous arrivons à 89% avec 12 valeurs propres.  -->
<!-- Avec 15 valeurs propres nous arrivons à 96%. -->
 
 

{% highlight r %}
#scree plot
# plot(acp1)
{% endhighlight %}
 
Nuage des points (les clusters) sur les deux premiers axes de composantes principales
 

{% highlight r %}
# create data frame with scores
scores = as.data.frame(acp1$scores)
 
# plot of observations
ggplot(data = scores, aes(x = Comp.1, y = Comp.2, label = rownames(scores))) +
  geom_hline(yintercept = 0, colour = "gray65") +
  geom_vline(xintercept = 0, colour = "gray65") +
  geom_text(colour = "tomato", alpha = 0.8, size = 4) +
  ggtitle("PCA plot")
{% endhighlight %}

![plot of chunk unnamed-chunk-8](/figures/unnamed-chunk-8-1.png)
 
<!-- Cercle de corrélations -->
 
<!-- ```{r} -->
 
 
<!-- # function to create a circle -->
<!-- circle <- function(center = c(0, 0), npoints = 100) { -->
<!--     r = 1 -->
<!--     tt = seq(0, 2 * pi, length = npoints) -->
<!--     xx = center[1] + r * cos(tt) -->
<!--     yy = center[1] + r * sin(tt) -->
<!--     return(data.frame(x = xx, y = yy)) -->
<!-- } -->
<!-- corcir = circle(c(0, 0), npoints = 100) -->
 
<!-- # create data frame with correlations between variables and PCs -->
<!-- correlations = as.data.frame(cor(clusters_scaled, acp1$scores)) -->
 
<!-- # data frame with arrows coordinates -->
<!-- arrows = data.frame(x1 = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0), y1 = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0), x2 = correlations$Comp.1,y2 = correlations$Comp.2) -->
 
<!-- # geom_path will do open circles -->
<!-- ggplot() + geom_path(data = corcir, aes(x = x, y = y), colour = "gray65") + -->
<!--     geom_segment(data = arrows, aes(x = x1, y = y1, xend = x2, yend = y2), colour = "gray65") + -->
<!--     geom_text(data = correlations, aes(x = Comp.1, y = Comp.2, label = rownames(correlations))) + -->
<!--     geom_hline(yintercept = 0, colour = "gray65") + geom_vline(xintercept = 0, -->
<!--     colour = "gray65") + xlim(-1.1, 1.1) + ylim(-1.1, 1.1) + labs(x = "pc1 aixs", -->
<!--     y = "pc2 axis") + ggtitle("Circle of correlations") -->
<!-- ``` -->
 
 
<!-- Les deux figures précédentes superposées -->
 

{% highlight r %}
#biplot
biplot(acp1)
{% endhighlight %}

![plot of chunk unnamed-chunk-9](/figures/unnamed-chunk-9-1.png)
 
Même figure mais différentes couleurs et thème
 

{% highlight r %}
# library("devtools")
# install_github("kassambara/factoextra")
library(factoextra)
fviz_pca_biplot(acp1)
{% endhighlight %}

![plot of chunk unnamed-chunk-10](/figures/unnamed-chunk-10-1.png)
 
 
 
 
 

{% highlight r %}
library(ggfortify)
autoplot(acp1,label = TRUE, loadings = TRUE, loadings.label = TRUE)
{% endhighlight %}

![plot of chunk unnamed-chunk-11](/figures/unnamed-chunk-11-1.png)
 
 
# KMEANS
 
## Nombre de clusters
 

{% highlight r %}
data_for_clustering <- clusters[,2:ncol(clusters)]
wss <- (nrow(data_for_clustering)-1)*sum(apply(data_for_clustering,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(data_for_clustering, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
{% endhighlight %}

![plot of chunk unnamed-chunk-12](/figures/unnamed-chunk-12-1.png)
 
## Kmeans avec 4 groupes :
 
 

{% highlight r %}
#extraire les comp princ pour visualiser
pc.comp <- acp1$scores
pc.comp1 <- -1*pc.comp[,1] # principal component 1 scores (negated for convenience)
pc.comp2 <- -1*pc.comp[,2]
 
 
X <- cbind(pc.comp1, pc.comp2)
 
cl <- kmeans(data_for_clustering,4)
res <- cbind(clusterId=clusters[,1],kmeans_clusterId=cl$cluster,X)
{% endhighlight %}
 
 
résultats du kmeans avec 4 clusters
 

{% highlight r %}
res <- as.data.frame(res)
qplot(as.factor(res$kmeans_clusterId))
{% endhighlight %}

![plot of chunk unnamed-chunk-14](/figures/unnamed-chunk-14-1.png)
 
Le nombre de ASI dans chaque groupes : 
 

{% highlight r %}
pander(table(res$kmeans_clusterId))
{% endhighlight %}



 1   2   3   4 
--- --- --- ---
83   4   4   9 
 
 
les clusters obtenus avec le kmeans, le nuage de points correpondant à ce qu'on a obtenu avec l'ACP
 

{% highlight r %}
ggplot(res, aes(pc.comp1, pc.comp2,label=clusterId)) +
  geom_point(aes(colour = factor(kmeans_clusterId),shape = factor(kmeans_clusterId)), size =3) +
  geom_text(aes(label=clusterId,colour = factor(kmeans_clusterId)),hjust=0, vjust=0, nudge_x = 0.1)
{% endhighlight %}

![plot of chunk unnamed-chunk-16](/figures/unnamed-chunk-16-1.png)
 

{% highlight r %}
ordered_clusters_ids <- order(table(res$kmeans_clusterId),decreasing=T)
{% endhighlight %}
 
 
## Quelques ASI du cluster 1  (15 ASIs): 
 

{% highlight r %}
#largest cluster first
 
# ordered_clusters_ids <- order(table(res$kmeans_clusterId),decreasing=T)
par(mfrow=c(1,1))
par(mfrow=c(8,2))
# par(mar=c(4,3,2,1))
cluster1 <- subset(res, kmeans_clusterId == ordered_clusters_ids[1])
ids_clusters <- sample(cluster1$clusterId,15)
 
for(ii in ids_clusters){
  subset <- data[clusterID == ii]
  
  #extraction de dates pour créer ASI (Activity Signal of Interest)
  dates <- as.POSIXct(subset$date,format="%d/%m/%Y %H:%M")
  rounded_dates <- round(dates , "hour")
  rounded_dates <- as.POSIXct(rounded_dates)
  Period_days <- difftime(max(rounded_dates),min(rounded_dates))
  Period_days <- round(Period_days,2)
  Period_hours <- difftime(max(rounded_dates),min(rounded_dates), units = "hours")
  Period_hours <- round(Period_hours,2)
  Occurences <- length(rounded_dates)
  
  freq <- plyr::count(rounded_dates)
  
  df1.zoo<-zoo(freq[,-1],freq[,1]) #set date to Index
  
  df2 <- merge(df1.zoo,zoo(,seq(start(df1.zoo),end(df1.zoo),by="hour")), all=TRUE)
  
  ASI <- data.frame(Date=time(df2), df2, check.names=FALSE, row.names=NULL)
  colnames(ASI) <- c("date", "freq")
  rm(df1.zoo,df2)
  ASI[is.na(ASI)] <- 0
 
  plot(ASI$date,ASI$freq,type="b",col="blue",xlab="",ylab="")
  mtext(paste("ASI",ii,"\n","T = ",Period_days, " days \n","#Occurences = ", Occurences, " posts" ))
 
#   print(
#   ggplot(ASI, aes(x=as.character(date), y=freq,group=1)) +
#     geom_line(color="blue") +
#     geom_point(color="blue") +
#     expand_limits(y=0) +
#     theme_minimal() +
#     ggtitle(paste("ASI of cluster",i,sep=" ")) +
#     xlab("time") +
#     theme(axis.text.x = element_blank())  +
#     annotate("text", color= "blue", size = 5, x=Inf, y = Inf, label = paste("T = ",Period_days, " days \n", "T = ", Period_hours, " hours \n", "#Occurences = ", Occurences, " posts" ), vjust=1, hjust=1)
# )
}
{% endhighlight %}

![plot of chunk unnamed-chunk-18](/figures/unnamed-chunk-18-1.png)
 
 
## Quelques ASI du cluster 4 : 
 

{% highlight r %}
cluster2 <- subset(res, kmeans_clusterId == ordered_clusters_ids[2])
par(mfrow=c(1,1))
par(mfrow=c(nrow(cluster2)/2,2))
 
ids_clusters <- cluster2$clusterId
 
for(ii in ids_clusters){
  subset <- data[clusterID == ii]
  
  #extraction de dates pour créer ASI (Activity Signal of Interest)
  dates <- as.POSIXct(subset$date,format="%d/%m/%Y %H:%M")
  rounded_dates <- round(dates , "hour")
  rounded_dates <- as.POSIXct(rounded_dates)
  Period_days <- difftime(max(rounded_dates),min(rounded_dates))
  Period_days <- round(Period_days,2)
  Period_hours <- difftime(max(rounded_dates),min(rounded_dates), units = "hours")
  Period_hours <- round(Period_hours,2)
  Occurences <- length(rounded_dates)
  
  freq <- plyr::count(rounded_dates)
  
  df1.zoo<-zoo(freq[,-1],freq[,1]) #set date to Index
  
  df2 <- merge(df1.zoo,zoo(,seq(start(df1.zoo),end(df1.zoo),by="hour")), all=TRUE)
  
  ASI <- data.frame(Date=time(df2), df2, check.names=FALSE, row.names=NULL)
  colnames(ASI) <- c("date", "freq")
  rm(df1.zoo,df2)
  ASI[is.na(ASI)] <- 0
 
  plot(ASI$date,ASI$freq,type="b",col="blue",xlab="",ylab="")
  mtext(paste("ASI",ii,"\n","T = ",Period_days, " days \n","#Occurences = ", Occurences, " posts" ))
 
#   print(
#   ggplot(ASI, aes(x=as.character(date), y=freq,group=1)) +
#     geom_line(color="blue") +
#     geom_point(color="blue") +
#     expand_limits(y=0) +
#     theme_minimal() +
#     ggtitle(paste("ASI of cluster",i,sep=" ")) +
#     xlab("time") +
#     theme(axis.text.x = element_blank())  +
#     annotate("text", color= "blue", size = 5, x=Inf, y = Inf, label = paste("T = ",Period_days, " days \n", "T = ", Period_hours, " hours \n", "#Occurences = ", Occurences, " posts" ), vjust=1, hjust=1)
# )
}
{% endhighlight %}

![plot of chunk unnamed-chunk-19](/figures/unnamed-chunk-19-1.png)![plot of chunk unnamed-chunk-19](/figures/unnamed-chunk-19-2.png)
 
## Quelques ASI du cluster 2 : 
 

{% highlight r %}
cluster3 <- subset(res, kmeans_clusterId == ordered_clusters_ids[3])
par(mfrow=c(1,1))
par(mfrow=c(nrow(cluster3)/2,2))
 
ids_clusters <- cluster3$clusterId
 
for(ii in ids_clusters){
  subset <- data[clusterID == ii]
  
  #extraction de dates pour créer ASI (Activity Signal of Interest)
  dates <- as.POSIXct(subset$date,format="%d/%m/%Y %H:%M")
  rounded_dates <- round(dates , "hour")
  rounded_dates <- as.POSIXct(rounded_dates)
  Period_days <- difftime(max(rounded_dates),min(rounded_dates))
  Period_days <- round(Period_days,2)
  Period_hours <- difftime(max(rounded_dates),min(rounded_dates), units = "hours")
  Period_hours <- round(Period_hours,2)
  Occurences <- length(rounded_dates)
  
  freq <- plyr::count(rounded_dates)
  
  df1.zoo<-zoo(freq[,-1],freq[,1]) #set date to Index
  
  df2 <- merge(df1.zoo,zoo(,seq(start(df1.zoo),end(df1.zoo),by="hour")), all=TRUE)
  
  ASI <- data.frame(Date=time(df2), df2, check.names=FALSE, row.names=NULL)
  colnames(ASI) <- c("date", "freq")
  rm(df1.zoo,df2)
  ASI[is.na(ASI)] <- 0
 
  plot(ASI$date,ASI$freq,type="b",col="blue",xlab="",ylab="")
  mtext(paste("ASI",ii,"\n","T = ",Period_days, " days \n","#Occurences = ", Occurences, " posts" ))
 
#   print(
#   ggplot(ASI, aes(x=as.character(date), y=freq,group=1)) +
#     geom_line(color="blue") +
#     geom_point(color="blue") +
#     expand_limits(y=0) +
#     theme_minimal() +
#     ggtitle(paste("ASI of cluster",i,sep=" ")) +
#     xlab("time") +
#     theme(axis.text.x = element_blank())  +
#     annotate("text", color= "blue", size = 5, x=Inf, y = Inf, label = paste("T = ",Period_days, " days \n", "T = ", Period_hours, " hours \n", "#Occurences = ", Occurences, " posts" ), vjust=1, hjust=1)
# )
}
{% endhighlight %}

![plot of chunk unnamed-chunk-20](/figures/unnamed-chunk-20-1.png)
 
## Quelques ASI du cluster 3 : 
 

{% highlight r %}
cluster4 <- subset(res, kmeans_clusterId == ordered_clusters_ids[4])
par(mfrow=c(1,1))
par(mfrow=c(nrow(cluster4)/2,2))
 
ids_clusters <- cluster4$clusterId
 
for(ii in ids_clusters){
  subset <- data[clusterID == ii]
  
  #extraction de dates pour créer ASI (Activity Signal of Interest)
  dates <- as.POSIXct(subset$date,format="%d/%m/%Y %H:%M")
  rounded_dates <- round(dates , "hour")
  rounded_dates <- as.POSIXct(rounded_dates)
  Period_days <- difftime(max(rounded_dates),min(rounded_dates))
  Period_days <- round(Period_days,2)
  Period_hours <- difftime(max(rounded_dates),min(rounded_dates), units = "hours")
  Period_hours <- round(Period_hours,2)
  Occurences <- length(rounded_dates)
  
  freq <- plyr::count(rounded_dates)
  
  df1.zoo<-zoo(freq[,-1],freq[,1]) #set date to Index
  
  df2 <- merge(df1.zoo,zoo(,seq(start(df1.zoo),end(df1.zoo),by="hour")), all=TRUE)
  
  ASI <- data.frame(Date=time(df2), df2, check.names=FALSE, row.names=NULL)
  colnames(ASI) <- c("date", "freq")
  rm(df1.zoo,df2)
  ASI[is.na(ASI)] <- 0
 
  plot(ASI$date,ASI$freq,type="b",col="blue",xlab="",ylab="")
  mtext(paste("ASI",ii,"\n","T = ",Period_days, " days \n","#Occurences = ", Occurences, " posts" ))
 
#   print(
#   ggplot(ASI, aes(x=as.character(date), y=freq,group=1)) +
#     geom_line(color="blue") +
#     geom_point(color="blue") +
#     expand_limits(y=0) +
#     theme_minimal() +
#     ggtitle(paste("ASI of cluster",i,sep=" ")) +
#     xlab("time") +
#     theme(axis.text.x = element_blank())  +
#     annotate("text", color= "blue", size = 5, x=Inf, y = Inf, label = paste("T = ",Period_days, " days \n", "T = ", Period_hours, " hours \n", "#Occurences = ", Occurences, " posts" ), vjust=1, hjust=1)
# )
}
{% endhighlight %}

![plot of chunk unnamed-chunk-21](/figures/unnamed-chunk-21-1.png)
 
 
## Kmeans avec 3 groupes :
 

{% highlight r %}
#extraire les comp princ pour visualiser
pc.comp <- acp1$scores
pc.comp1 <- -1*pc.comp[,1] # principal component 1 scores (negated for convenience)
pc.comp2 <- -1*pc.comp[,2]
 
 
X <- cbind(pc.comp1, pc.comp2)
 
cl <- kmeans(data_for_clustering,3)
res <- cbind(clusterId=clusters[,1],kmeans_clusterId=cl$cluster,X)
{% endhighlight %}
 
résultats du kmeans avec 3 clusters
 

{% highlight r %}
res <- as.data.frame(res)
qplot(as.factor(res$kmeans_clusterId))
{% endhighlight %}

![plot of chunk unnamed-chunk-23](/figures/unnamed-chunk-23-1.png)
 
 
les clusters obtenus avec le kmeans, le nuage de points correpondant à ce qu'on a obtenu avec l'ACP
 

{% highlight r %}
ggplot(res, aes(pc.comp1, pc.comp2,label=clusterId)) +
  geom_point(aes(colour = factor(kmeans_clusterId),shape = factor(kmeans_clusterId)), size =3) +
  geom_text(aes(label=clusterId,colour = factor(kmeans_clusterId)),hjust=0, vjust=0, nudge_x = 0.1)
{% endhighlight %}

![plot of chunk unnamed-chunk-24](/figures/unnamed-chunk-24-1.png)
 

{% highlight r %}
# fviz_cluster(cl, data = clusters)+theme_minimal()+
#   scale_color_manual(values = c("#00AFBB","#2E9FDF", "#E7B800"))+
#   scale_fill_manual(values = c("#00AFBB","#2E9FDF", "#E7B800")) +
#   labs(title= "Partitioning Clustering Plot")
{% endhighlight %}
 
##Commentaires :
 
 
 
# Suite
 
Nuage de points et cercle de corrélations sur les autres axes, les couleurs correspondent aux groupes obtenus par kmeans :
 
Axes 1 et 2 :
 

{% highlight r %}
fviz_pca_biplot(acp1, label = c("ind","var"), habillage = res$kmeans_clusterId,col.ind.sup = "gray") +
  theme_minimal()
{% endhighlight %}

![plot of chunk unnamed-chunk-26](/figures/unnamed-chunk-26-1.png)
 
Axes 1 et 3 :
 

{% highlight r %}
fviz_pca_biplot(acp1, label = c("ind","var"), axes = c(1,3),habillage = res$kmeans_clusterId,col.ind.sup = "gray") +
  theme_minimal()
{% endhighlight %}

![plot of chunk unnamed-chunk-27](/figures/unnamed-chunk-27-1.png)
 
Axes 2 et 3 :
 

{% highlight r %}
fviz_pca_biplot(acp1, label = c("ind","var"), axes = c(2,3),habillage = res$kmeans_clusterId,col.ind.sup = "gray") +
  theme_minimal()
{% endhighlight %}

![plot of chunk unnamed-chunk-28](/figures/unnamed-chunk-28-1.png)
 
Contribution des individus, Axe 1 :
 

{% highlight r %}
fviz_contrib(acp1, choice = "ind", axes = 1)
{% endhighlight %}

![plot of chunk unnamed-chunk-29](/figures/unnamed-chunk-29-1.png)
 
Contribution des individus, Axe 2 :

{% highlight r %}
fviz_contrib(acp1, choice = "ind", axes = 2)
{% endhighlight %}

![plot of chunk unnamed-chunk-30](/figures/unnamed-chunk-30-1.png)
 
 
Contribution des individus, Axe 3 :

{% highlight r %}
fviz_contrib(acp1, choice = "ind", axes = 3)
{% endhighlight %}

![plot of chunk unnamed-chunk-31](/figures/unnamed-chunk-31-1.png)
 
 
Contribution variables (Axe 1) :

{% highlight r %}
fviz_contrib(acp1, choice = "var", axes = 1)
{% endhighlight %}

![plot of chunk unnamed-chunk-32](/figures/unnamed-chunk-32-1.png)
 
Contribution variables (Axe 2) :

{% highlight r %}
fviz_contrib(acp1, choice = "var", axes = 2)
{% endhighlight %}

![plot of chunk unnamed-chunk-33](/figures/unnamed-chunk-33-1.png)
 
Contribution variables (Axe 3) :

{% highlight r %}
fviz_contrib(acp1, choice = "var", axes = 3)
{% endhighlight %}

![plot of chunk unnamed-chunk-34](/figures/unnamed-chunk-34-1.png)
 
 
 
 
 
 
 
 
